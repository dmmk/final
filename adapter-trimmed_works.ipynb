{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11905691,"sourceType":"datasetVersion","datasetId":7484217}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os # Import the os module\n\n# Define the base folder in Google Drive for data and outputs\n# This should be the folder containing your CSV data\nbase_drive_folder = \"/kaggle/input/dataset1\" # Keep this for data and general outputs\nbase_drive_folder_output = \"/kaggle/working/\" # Keep this for data and general outputs\n\n# Define the directory where the trimmed model and tokenizer were saved by vocabtrimmer\n# IMPORTANT: Update this path if you saved the trimmed model to a different location\ntrimmed_model_path = \"/kaggle/input/dataset1\"\nprint(f\"Attempting to load trimmed model and tokenizer from: {trimmed_model_path}\")\n\n# Create the base output folder if it doesn't exist\nos.makedirs(base_drive_folder, exist_ok=True)\nprint(f\"Training outputs (checkpoints, logs, results) will be saved to: {base_drive_folder}\")\n\n# Step 1: Setup and Installation\n# Install necessary libraries\n# Removed peft and added adapters\n!pip install --upgrade transformers\n!pip install --upgrade datasets\n!pip install adapters accelerate evaluate rouge_score nltk pandas rawpy Pillow tensorboard\n\n# Print transformers version to verify installation\nimport transformers\nprint(f\"Transformers version: {transformers.__version__}\")\n\n# Note: Meteor also requires Java to be installed on your system.\n# If you still get errors for Meteor, you might need to install Java.\n# Restart your Colab runtime after installation if prompted.\n\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport numpy as np\nimport gc # Import garbage collector\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, TrainerCallback\n# CORRECTED: Import AdapterConfig only, and initialize adapters for the model\nfrom adapters import AdapterConfig\nimport adapters # Import the adapters library to patch AutoModel classes\n\nfrom datasets import load_dataset, DatasetDict, Dataset\n\nimport evaluate\nimport nltk\n\n# Ensure necessary nltk data is downloaded for metrics\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"wordnet\", quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger', quiet=True) # sometimes needed for meteor, but keeping for now as it's small\n\n# Step 2: Load the Trimmed Model and Tokenizer\n# Load the model and tokenizer from the directory where vocabtrimmer saved them\ntry:\n    print(f\"\\nLoading trimmed model from {trimmed_model_path}...\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(trimmed_model_path)\n    print(f\"Loading trimmed tokenizer from {trimmed_model_path}...\")\n    tokenizer = AutoTokenizer.from_pretrained(trimmed_model_path)\n    print(\"Trimmed model and tokenizer loaded successfully!\")\n    print(f\"Loaded model vocabulary size: {model.config.vocab_size}\")\n    print(f\"Loaded tokenizer vocabulary size: {len(tokenizer)}\")\n\n    # Initialize adapters for the model AFTER loading it\n    # This patches the model to enable adapter functionalities\n    adapters.init(model)\n    print(\"Adapters initialized for the trimmed model.\")\n\nexcept Exception as e:\n    print(f\"Error loading trimmed model or tokenizer from {trimmed_model_path}: {e}\")\n    print(\"Please ensure the trimmed model and tokenizer were saved correctly by vocabtrimmer\")\n    print(\"and that the 'trimmed_model_path' variable points to the correct directory.\")\n    print(\"Exiting script.\")\n    raise SystemExit(\"Failed to load trimmed model/tokenizer.\")\n\n\n# Check for CUDA availability and move model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"Model moved to device: {device}\")\n\n\n# Step 3: Load and Preprocess Your Data (Using three CSV files from Google Drive)\n\n# Define the paths to your CSV files within the Google Drive folder\ntrain_csv_path = os.path.join(base_drive_folder, 'amharic_3_train.csv')\nvalid_csv_path = os.path.join(base_drive_folder, 'amharic_3_valid.csv')\ntest_csv_path = os.path.join(base_drive_folder, 'amharic_3_test.csv')\n\n# IMPORTANT: Make sure you have uploaded amharic_3_train.csv, amharic_3_valid.csv,\n# and amharic_3_test.csv into the specified base_drive_folder in your Google Drive.\n\n# Load each dataset split from its respective CSV file\ntry:\n    # load_dataset returns a DatasetDict, get the 'train' split for each file\n    train_dataset = load_dataset('csv', data_files=train_csv_path)['train']\n    valid_dataset = load_dataset('csv', data_files=valid_csv_path)['train']\n    test_dataset = load_dataset('csv', data_files=test_csv_path)['train']\n\n    # Select only the 'text' and 'summary' columns for each dataset\n    train_dataset = train_dataset.select_columns(['text', 'summary'])\n    valid_dataset = valid_dataset.select_columns(['text', 'summary'])\n    test_dataset = test_dataset.select_columns(['text', 'summary'])\n\n    # Combine into a DatasetDict\n    dataset = DatasetDict({\n        'train': train_dataset,\n        'validation': valid_dataset,\n        'test': test_dataset\n    })\n    print(\"\\nDataset loaded from CSV files.\")\n\nexcept FileNotFoundError:\n    print(\"\\nError: One or more of the specified CSV files were not found in Google Drive.\")\n    print(f\"Please make sure amharic_3_train.csv, amharic_3_valid.csv, and amharic_3_test.csv\")\n    print(f\"are uploaded to your Google Drive folder: {base_drive_folder}\")\n    # Create dummy datasets for demonstration if files not found\n    print(\"Creating dummy datasets for demonstration.\")\n    dummy_train_data = {\"text\": [\"Train document one for dummy data.\", \"Train document two for dummy data.\"], \"summary\": [\"Train sum 1.\", \"Train sum 2.\"]}\n    dummy_valid_data = {\"text\": [\"Valid document one for dummy data.\"], \"summary\": [\"Valid sum 1.\"]}\n    dummy_test_data = {\"text\": [\"Test document one for dummy data.\"], \"summary\": [\"Test sum 1.\"]}\n    dataset = DatasetDict({\n        'train': Dataset.from_dict(dummy_train_data),\n        'validation': Dataset.from_dict(dummy_valid_data),\n        'test': Dataset.from_dict(dummy_test_data)\n    })\n\n\nprint(\"Dataset sizes:\")\nprint(f\"Train size: {len(dataset['train'])}\")\nprint(f\"Validation size: {len(dataset['validation'])}\")\nprint(f\"Test size: {len(dataset['test'])}\")\n\n# Define the preprocessing function (DataCollator will handle padding)\n# Use the loaded (trimmed) tokenizer\ndef preprocess_function(examples):\n    # Tokenize documents (text column)\n    # Adjust max_length based on the typical length of your documents\n    model_inputs = tokenizer(\n        examples['text'],\n        max_length=512, # Max length for inputs\n        truncation=True,\n        # padding is handled by DataCollatorForSeq2Seq\n    )\n\n    # Tokenize summaries (summary column)\n    # Adjust max_length based on the typical length of your summaries\n    labels = tokenizer(\n        examples['summary'],\n        max_length=128, # Max length for labels\n        truncation=True,\n        # padding is handled by DataCollatorForSeq2Seq\n    )\n\n    # Assign input_ids of summaries as labels\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n    return model_inputs\n\n# Apply the preprocessing function to all splits\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\n\n# Set the format of the datasets to PyTorch tensors\n# Specify which columns should be converted to tensors\ntokenized_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n\n# Now, the variables for each split are directly available\ntokenized_train_dataset = tokenized_dataset['train']\ntokenized_eval_dataset = tokenized_dataset['validation']\ntokenized_test_dataset = tokenized_dataset['test']\n\n# Step 4: Configure and Apply Adapters (Replaced LoRA)\nprint(\"\\nConfiguring and applying Adapters...\")\n# Define adapter configuration\nadapter_name = \"amharic_summarization_adapter\"\n# CORRECTED: Load adapter configuration using AdapterConfig.load()\n# \"double_seq_bn\" is the configuration name for the Houlsby adapter type\nadapter_config = AdapterConfig.load(\"double_seq_bn\")\n\n# Add the adapter to the model using the loaded configuration object\nmodel.add_adapter(adapter_name, config=adapter_config)\n\n# Set the active adapter for training\nmodel.set_active_adapters(adapter_name)\n\n# Enable adapter training: This freezes the base model parameters and only trains the adapter layers\nmodel.train_adapter(adapter_name)\n\n# Print trainable parameters (will now show adapter parameters)\nprint(\"\\nTrainable parameters (Adapters on Trimmed Model):\")\n#model.print_trainable_parameters()\n# Print trainable parameters (Manual Calculation with generator expression)\nprint(\"\\nTrainable parameters (Adapter baseline):\")\n# Calculate total trainable parameters using a generator expression\ntotal_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Calculate total parameters (optional, for context)\ntotal_all_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"Total parameters: {total_all_params}\")\nprint(f\"Trainable parameters: {total_trainable_params}\")\n# Calculate and print the percentage of trainable parameters\nif total_all_params > 0:\n    print(f\"Percentage of trainable parameters: {100 * total_trainable_params / total_all_params:.2f}%\")\nelse:\n    print(\"Percentage of trainable parameters: 0.00%\")\n\n\n# --- Custom Callback for Cache Clearing ---\nclass ClearCacheCallback(TrainerCallback):\n    \"\"\"\n    A TrainerCallback that clears the CUDA cache before evaluation steps.\n    Using on_step_end hook to check if evaluation is about to happen.\n    Includes gc.collect().\n    \"\"\"\n    def on_step_end(self, args, state, control, **kwargs):\n        # Check if evaluation is scheduled for the next step\n        # This condition is true when state.global_step is a multiple of args.eval_steps\n        # and state.global_step > 0\n        if args.eval_strategy == \"steps\" and state.global_step > 0 and args.eval_steps and state.global_step % args.eval_steps == 0:\n             if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                # Also try garbage collection\n                gc.collect()\n                print(f\"\\nCUDA cache cleared and gc.collect() called by callback before evaluation at step {state.global_step}.\")\n                # Set control.should_evaluate to True to ensure evaluation happens right after this step end\n                # This might already be handled by the Trainer, but explicit can help sometimes.\n                control.should_evaluate = True\n\n# --- Custom Trainer to filter unexpected arguments ---\nclass FilteredArgsSeq2SeqTrainer(Seq2SeqTrainer):\n    # Override training_step to filter inputs before model call\n    # Updated signature to accept num_items_in_batch\n    def training_step(self, model, inputs, num_items_in_batch=None):\n        # Remove 'num_items_in_batch' if it's present in inputs (belt-and-suspenders approach)\n        if 'num_items_in_batch' in inputs:\n            del inputs['num_items_in_batch']\n        # Also explicitly ignore the positional argument if it was passed\n        if num_items_in_batch is not None:\n             # You could potentially log a warning here if you want to know this happened\n             pass # Simply ignore the argument\n\n        # Call the parent class's training_step with the filtered inputs\n        return super().training_step(model, inputs)\n\n    # Override prediction_step to filter inputs before model call\n    # Updated signature to accept num_items_in_batch\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None, num_items_in_batch=None):\n         # Remove 'num_items_in_batch' if it's present in inputs (belt-and-suspenders approach)\n        if 'num_items_in_batch' in inputs:\n            del inputs['num_items_in_batch']\n         # Also explicitly ignore the positional argument if it was passed\n        if num_items_in_batch is not None:\n             # You could potentially log a warning here if you want to know this happened\n             pass # Simply ignore the argument\n\n        # Call the parent class's prediction_step with the filtered inputs\n        return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n\n\n\n# Step 5: Define Training Arguments and Trainer (Configured for GPU with Drive Outputs)\n\n# Define the output directory within your Google Drive for training logs and checkpoints\n# Using the base_drive_folder for training outputs\ntraining_output_dir = os.path.join(base_drive_folder_output, \"trimmed_adapters_checkpoints\") # CHANGED output directory name\n# Create the directory if it doesn't exist\nos.makedirs(training_output_dir, exist_ok=True)\nprint(f\"Training outputs (checkpoints, logs) will be saved to: {training_output_dir}\")\n\n\n# Instantiate Data Collator (Handles padding batches dynamically)\n# We pass the tokenizer so it knows the pad token ID\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model) # Pass model (Adapter model on trimmed base) here\n\n# Define training arguments (Revised for GPU with Drive Checkpointing and TensorBoard)\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=training_output_dir, # Output directory for checkpoints and logs (in Drive)\n    num_train_epochs=3, # Number of training epochs - Adjust based on convergence\n    per_device_train_batch_size=2, # Batch size per device during training\n    per_device_eval_batch_size=1, # Evaluation batch size\n    gradient_accumulation_steps=2, # Gradient accumulation steps to compensate for smaller batch size\n    learning_rate=1e-5, # Keep reduced learning rate for stability\n    weight_decay=0.01, # Weight decay - Adjust based on experimentation\n    eval_strategy=\"steps\", # Evaluate every N steps to match save_strategy\n    eval_steps=500, # Evaluate every 500 training steps (matches save_steps)\n    save_strategy=\"steps\", # Save checkpoint every N steps\n    save_steps=500, # Save a checkpoint every 500 training steps\n    save_total_limit=3, # Optional: Limit the total number of checkpoints to save\n    load_best_model_at_end=True, # Load the best model at the end of training based on metric_for_best_model\n    metric_for_best_model=\"rougeL\", # Metric to monitor for best model - Change if needed\n    greater_is_better=True, # Set to False for metrics like loss\n    report_to=\"tensorboard\", # Log metrics to TensorBoard\n    push_to_hub=False, # Set to True if you want to push to Hugging Face Hub\n    label_names=[\"labels\"], # Explicitly tell the Trainer your label column name\n    fp16=False, # Keep mixed precision training disabled for stability\n    # bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(), # Use bf16 if GPU supports it\n)\n\n# Define compute_metrics function for evaluation\nmetric_rouge = evaluate.load(\"rouge\")\nmetric_bleu = evaluate.load(\"bleu\")\n\ndef compute_metrics(eval_pred):\n    predictions = eval_pred.predictions\n    labels = eval_pred.label_ids\n\n    # Debugging prints to inspect types and shapes\n    print(\"\\n--- Inside compute_metrics (DEBUG) ---\")\n    print(f\"Type of eval_pred.predictions (initial): {type(eval_pred.predictions)}\")\n    print(f\"Type of eval_pred.label_ids (initial): {type(eval_pred.label_ids)}\")\n\n    # CRITICAL FIX: If predictions is a tuple (e.g., logits and hidden states), take the first element (logits)\n    if isinstance(predictions, tuple):\n        print(f\"eval_pred.predictions is a tuple. Length: {len(predictions)}\")\n        predictions = predictions[0] # Assume the first element is the actual generated sequences (logits)\n        print(f\"Taking first element of predictions tuple. New type: {type(predictions)}\")\n\n    # Ensure predictions and labels are NumPy arrays\n    if isinstance(predictions, torch.Tensor):\n        predictions = predictions.cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.cpu().numpy()\n\n    print(f\"Predictions type (after initial processing): {type(predictions)}\")\n    print(f\"Labels type (after initial processing): {type(labels)}\")\n\n    # CRITICAL FIX: If predictions are logits (3D array), convert to token IDs using argmax\n    if predictions.ndim == 3:\n        print(f\"WARNING: Predictions has unexpected 3 dimensions. Assuming logits and applying argmax.\")\n        predictions = np.argmax(predictions, axis=-1) # Convert logits to token IDs\n        print(f\"Predictions after argmax: New shape: {predictions.shape}, New dtype: {predictions.dtype}\")\n\n    # Further check for dimensions: if predictions is 3D (e.g., batch_size, seq_len, 1), squeeze it\n    # This handles cases where the model output might have an extra dimension of size 1\n    if predictions.ndim == 3 and predictions.shape[-1] == 1:\n        predictions = predictions.squeeze(-1)\n        print(f\"Predictions was 3D with last dim 1, squeezed to 2D. New shape: {predictions.shape}\")\n    elif predictions.ndim > 2:\n        # If it's more than 2D and not just a trailing 1, this is unexpected for token IDs\n        print(f\"WARNING: Predictions has unexpected {predictions.ndim} dimensions (after argmax check). Expected 2D.\")\n        predictions = predictions.reshape(predictions.shape[0], -1) # Attempt to flatten if possible\n        print(f\"Attempted to reshape predictions to 2D. New shape: {predictions.shape}\")\n\n    # Ensure predictions are integers (important for tokenizer.batch_decode)\n    if predictions.dtype != np.int64 and predictions.dtype != np.int32:\n        print(f\"WARNING: Predictions dtype is {predictions.dtype}, converting to int64.\")\n        predictions = predictions.astype(np.int64)\n    if labels.dtype != np.int64 and labels.dtype != np.int32:\n        print(f\"WARNING: Labels dtype is {labels.dtype}, converting to int64.\")\n        labels = labels.astype(np.int64)\n\n    print(f\"Predictions shape (after processing): {predictions.shape}\")\n    print(f\"Labels shape (after processing): {labels.shape}\")\n\n    # Replace -100 in labels as they are ignored in loss calculation\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    print(\"Labels processed (replaced -100).\")\n\n    try:\n        # Decode predictions and labels, removing special tokens\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        print(f\"Successfully decoded {len(decoded_preds)} predictions and {len(decoded_labels)} labels.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to decode predictions/labels in compute_metrics: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise # Re-raise to crash session with specific error\n\n    # FIX: Replace empty strings with a single character 'a' to prevent ZeroDivisionError in BLEU\n    # This is crucial for metrics like BLEU that perform length calculations.\n    decoded_preds_for_metrics = [pred if pred else \"a\" for pred in decoded_preds]\n    decoded_labels_for_metrics = [label if label else \"a\" for label in decoded_labels]\n    print(\"Empty strings in decoded predictions/labels replaced with 'a' for robust metric calculation.\")\n\n    # Some metrics expect a list of lists for references\n    formated_decoded_labels_for_some_metrics = [[label] for label in decoded_labels_for_metrics] # Use the cleaned lists for metrics\n    print(\"Labels formatted for metrics.\")\n\n    # --- Compute Metrics ---\n    try:\n        # Rouge\n        rouge_results = metric_rouge.compute(predictions=decoded_preds_for_metrics, references=decoded_labels_for_metrics)\n        print(\"ROUGE computed.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to compute ROUGE in compute_metrics: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise # Re-raise\n\n    try:\n        # BLEU (requires references as list of lists)\n        bleu_results = metric_bleu.compute(predictions=decoded_preds_for_metrics, references=formated_decoded_labels_for_some_metrics)\n        print(\"BLEU computed.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to compute BLEU in compute_metrics: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise # Re-raise\n\n    # Combine metrics - only include ROUGE and BLEU\n    combined_results = {\n        \"rouge1\": rouge_results[\"rouge1\"], # F1 score is commonly reported\n        \"rouge2\": rouge_results[\"rouge2\"],\n        \"rougeL\": rouge_results[\"rougeL\"],\n        \"bleu\": bleu_results[\"bleu\"],\n    }\n    print(\"Metrics combined.\")\n    print(\"--- Exiting compute_metrics ---\")\n    # The Trainer expects a dictionary of metrics\n    return combined_results\n\n\n# Create Trainer instance, adding the custom callback\n# CHANGED: Use FilteredArgsSeq2SeqTrainer instead of Seq2SeqTrainer\ntrainer = FilteredArgsSeq2SeqTrainer(\n    model=model, # This is the Adapter model on the TRIMMED base model\n    args=training_args, # These are the GPU-configured args with Drive checkpointing and reporting\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_eval_dataset,\n    tokenizer=tokenizer, # Pass the trimmed tokenizer here\n    data_collator=data_collator, # Add the data collator here\n    compute_metrics=compute_metrics, # Pass the compute_metrics function\n    callbacks=[ClearCacheCallback()], # Add the custom cache clearing callback here\n)\n\n\n# Step 6: Train the Model & Measure Time (Includes Resuming from Checkpoint in Drive)\nprint(\"\\nStarting training (Adapters on Trimmed Model)...\") # CHANGED print statement\n\n# --- Resume Training Logic ---\n# Check if there's a checkpoint to resume from in the output directory\n# The Trainer saves checkpoints in subdirectories like 'checkpoint-XXXX'\nlatest_checkpoint_dir = None\nif os.path.isdir(training_args.output_dir):\n    # Find all checkpoint directories\n    checkpoints = [m for m in os.listdir(training_args.output_dir) if m.startswith('checkpoint-')]\n    if checkpoints:\n        # Sort checkpoints by step number to find the latest\n        latest_checkpoint_dir = os.path.join(training_args.output_dir, sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1])\n        print(f\"Found latest checkpoint: {latest_checkpoint_dir}\")\n\n# Set resume_from_checkpoint to the latest found checkpoint if it exists\nresume_from_checkpoint = latest_checkpoint_dir if latest_checkpoint_dir and os.path.isdir(latest_checkpoint_dir) else None\n\nif resume_from_checkpoint:\n     print(f\"Resuming training from checkpoint: {resume_from_checkpoint}\")\nelse:\n     print(\"No checkpoint found, starting fresh training.\")\n\nstart_time = time.time()\n\n# Start training (pass resume_from_checkpoint if a checkpoint was found)\ntrain_output = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\nend_time = time.time()\ntotal_training_time_seconds = end_time - start_time\ntotal_training_time_minutes = total_training_time_seconds / 60.0\n\nprint(f\"\\nTotal training time (Adapters on Trimmed Model): {total_training_time_seconds:.2f} seconds ({total_training_time_minutes:.2f} minutes)\") # CHANGED print statement\n\n\n# Step 7: Evaluate the Model on the Test Set and Get Metrics\nprint(\"\\n--- Starting Model Evaluation on Test Set ---\")\ntry:\n    # --- Explicitly Clear CUDA Cache Before Final Evaluation ---\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect() # Also call gc.collect() here\n        print(\"\\nCUDA cache cleared and gc.collect() called explicitly before final evaluation.\")\n    else:\n        print(\"\\nCUDA not available, skipping cache clear before final evaluation.\")\n\n    print(\"Calling trainer.evaluate...\")\n    evaluation_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n    print(\"trainer.evaluate completed.\")\n\n    print(\"\\nEvaluation Metrics (Adapters on Trimmed Model):\") # CHANGED print statement\n    print(evaluation_results)\n\n    # --- Save Evaluation Results to File ---\n    eval_results_file = os.path.join(base_drive_folder_output, \"trimmed_adapters_evaluation_results.json\") # CHANGED file name\n    with open(eval_results_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(evaluation_results, f, indent=4)\n    print(f\"\\nEvaluation results saved to: {eval_results_file}\")\n\nexcept Exception as e:\n    print(f\"\\nERROR during Step 7 (Model Evaluation): {e}\")\n    import traceback\n    traceback.print_exc() # Print full traceback for detailed error\n\n\n# Step 8: Plot Training vs Validation Loss for Convergence Analysis\nprint(\"\\n--- Starting Loss Curve Plotting ---\")\ntry:\n    # Extract loss and eval_loss from the trainer's log history\n    train_loss_entries = [entry for entry in trainer.state.log_history if 'loss' in entry]\n    eval_loss_entries = [entry for entry in trainer.state.log_history if 'eval_loss' in entry]\n\n    train_steps = [entry['step'] for entry in train_loss_entries]\n    train_losses = [entry['loss'] for entry in train_loss_entries]\n\n    # For evaluation, the step corresponds to the global step at which evaluation was performed\n    eval_steps = [entry['step'] for entry in eval_loss_entries]\n    eval_losses = [entry['eval_loss'] for entry in eval_loss_entries]\n\n    if train_steps and eval_steps:\n        plt.figure(figsize=(10, 6))\n        plt.plot(train_steps, train_losses, label=\"Training Loss\", marker='o', linestyle='-')\n        plt.plot(eval_steps, eval_losses, label=\"Validation Loss\", marker='s', linestyle='--')\n        plt.xlabel(\"Training Steps\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Training vs Validation Loss Over Steps (Adapters)\") # CHANGED plot title\n        plt.legend()\n        plt.grid(True)\n\n        # Save the plot to Google Drive\n        loss_plot_path = os.path.join(base_drive_folder_output, \"trimmed_adapters_loss_curve.png\") # CHANGED file name\n        plt.savefig(loss_plot_path)\n        print(f\"Loss curve plot saved to: {loss_plot_path}\")\n        plt.show()\n    else:\n        print(\"Not enough data in log history to plot loss curves. Ensure eval_strategy and logging_steps are configured.\")\nexcept Exception as e:\n    print(f\"\\nERROR during Step 8 (Loss Plotting): {e}\")\n    import traceback\n    traceback.print_exc()\n\n\n# Step 9: Save the Final (Best) Adapter Weights to Drive\nprint(\"\\n--- Starting Adapter Weights Saving ---\") # CHANGED print statement\ntry:\n    # After trainer.train() finishes (and because load_best_model_at_end=True),\n    # the model object in memory is the best model found during training.\n    # Save only the adapter weights, not the full trimmed base model\n    adapters_output_dir = os.path.join(base_drive_folder_output, \"trimmed_adapters_final_best_adapter\") # CHANGED output directory name\n    \n    # The adapters library provides a specific method to save only the adapter weights\n    model.save_adapter(adapters_output_dir, adapter_name) # Save only the specified adapter\n    print(f\"\\nFinal (best) adapter weights for '{adapter_name}' saved to {adapters_output_dir}\")\n\n    # To save the tokenizer along with the adapter weights (recommended):\n    tokenizer.save_pretrained(adapters_output_dir)\n    print(f\"Tokenizer saved to {adapters_output_dir}\")\n\n    # --- How to Load the Saved Trimmed Model with Adapter Weights Later ---\n    print(\"\\n--- How to Load the Saved Trimmed Model with Adapter Weights Later ---\")\n    print(f\"To load the trimmed base model and apply the saved adapter weights later, you would do:\")\n    print(f\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\")\n    print(f\"from adapters import AdapterConfig\") # CORRECTED import\n    print(f\"import adapters\") # CORRECTED import\n    print(f\"\")\n    print(f\"# Path to the directory where you saved the trimmed base model (from vocabtrimmer)\")\n    print(f\"trimmed_base_model_path = '{trimmed_model_path}'\")\n    print(f\"# Path to the directory where you saved the fine-tuned adapter weights\")\n    print(f\"saved_adapters_dir = '{adapters_output_dir}'\")\n    print(f\"adapter_name = '{adapter_name}'\")\n    print(f\"\")\n    print(f\"# Load the trimmed base model\")\n    print(f\"loaded_trimmed_model = AutoModelForSeq2SeqLM.from_pretrained(trimmed_base_model_path)\")\n    print(f\"\")\n    print(f\"# Initialize adapters for the loaded model\") # CORRECTED\n    print(f\"adapters.init(loaded_trimmed_model)\") # CORRECTED\n    print(f\"\")\n    print(f\"# Load the adapter weights and add them to the model\")\n    print(f\"# Use AdapterConfig.load() to get the correct config type\") # CORRECTED\n    print(f\"loaded_trimmed_model.load_adapter(saved_adapters_dir, config=AdapterConfig.load('double_seq_bn'), load_as=adapter_name)\") # CORRECTED\n    print(f\"\")\n    print(f\"# Set the adapter as active for inference\")\n    print(f\"loaded_trimmed_model.set_active_adapters(adapter_name)\")\n    print(f\"\")\n    print(f\"# Load the tokenizer\")\n    print(f\"loaded_tokenizer = AutoTokenizer.from_pretrained(saved_adapters_dir)\") # Tokenizer was saved with adapter weights\n    print(f\"\")\n    print(f\"# If running on GPU, ensure model is on GPU\")\n    print(f\"# import torch\")\n    print(f\"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\")\n    print(f\"# loaded_trimmed_model.to(device)\")\n    print(f\"\")\n    print(f\"# Set to evaluation mode\")\n    print(f\"# loaded_trimmed_model.eval()\")\n    print(f\"\")\n    print(f\"print(f'Successfully loaded trimmed model and applied adapter weights from {saved_adapters_dir}')\")\n    print(f\"print(f'Loaded model vocabulary size: {loaded_trimmed_model.config.vocab_size}')\")\n    print(f\"print(f'Loaded tokenizer vocabulary size: {len(loaded_tokenizer)}')\")\n    print(f\"\")\n    print(f\"# Example Inference (Optional)\")\n    print(f\"# text_to_summarize = 'የአማርኛ ጽሑፍ እዚህ ይገባል...'\")\n    print(f\"# inputs = loaded_tokenizer(text_to_summarize, return_tensors='pt', max_length=512, truncation=True).to(device)\")\n    print(f\"# output_tokens = loaded_trimmed_model.generate(**inputs, max_new_tokens=128, num_beams=4, early_stopping=True)\")\n    print(f\"# generated_summary = loaded_tokenizer.decode(output_tokens[0], skip_special_tokens=True)\")\n    print(f\"# print(f'Generated Summary: {generated_summary}')\")\n\nexcept Exception as e:\n    print(f\"\\nERROR during Step 9 (Adapter Saving): {e}\")\n    import traceback\n    traceback.print_exc()\n\n\n\n# Step 10: Display Example Summaries with ROUGE Scores\nprint(\"\\n--- Starting Example Summaries Generation ---\")\ntry:\n    # Get predictions for the test set\n    generated_predictions = trainer.predict(\n        test_dataset=tokenized_test_dataset,\n        max_new_tokens=128, # Max length of generated summary (matches labels)\n        num_beams=4, # Number of beams for beam search\n    )\n    print(\"trainer.predict for example summaries completed.\")\n\n    preds = generated_predictions.predictions\n    labels = generated_predictions.label_ids\n\n    # --- Start of FIX for Step 10 Decoding Error ---\n    print(\"\\n--- Inside Step 10 Decoding (DEBUG) ---\")\n    print(f\"Type of preds (initial): {type(preds)}\")\n    print(f\"Type of labels (initial): {type(labels)}\")\n\n    # CRITICAL FIX: If predictions is a tuple (e.g., logits and hidden states), take the first element (logits)\n    if isinstance(preds, tuple):\n        print(f\"preds is a tuple. Length: {len(preds)}\")\n        preds = preds[0] # Assume the first element is the actual generated sequences (logits)\n        print(f\"Taking first element of preds tuple. New type: {type(preds)}\")\n\n    # Ensure predictions and labels are NumPy arrays\n    if isinstance(preds, torch.Tensor):\n        preds = preds.cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.cpu().numpy()\n\n    print(f\"preds type (after initial processing): {type(preds)}\")\n    print(f\"labels type (after initial processing): {type(labels)}\")\n\n    # CRITICAL FIX: If predictions are logits (3D array), convert to token IDs using argmax\n    if preds.ndim == 3:\n        print(f\"WARNING: preds has unexpected 3 dimensions. Assuming logits and applying argmax.\")\n        preds = np.argmax(preds, axis=-1) # Convert logits to token IDs\n        print(f\"preds after argmax: New shape: {preds.shape}, New dtype: {preds.dtype}\")\n\n    # Further check for dimensions: if preds is 3D (e.g., batch_size, seq_len, 1), squeeze it\n    if preds.ndim == 3 and preds.shape[-1] == 1:\n        preds = preds.squeeze(-1)\n        print(f\"preds was 3D with last dim 1, squeezed to 2D. New shape: {preds.shape}\")\n    elif preds.ndim > 2:\n        print(f\"WARNING: preds has unexpected {preds.ndim} dimensions (after argmax check). Expected 2D.\")\n        preds = preds.reshape(preds.shape[0], -1) # Attempt to flatten if possible\n        print(f\"Attempted to reshape preds to 2D. New shape: {preds.shape}\")\n\n    # Ensure predictions are integers (important for tokenizer.batch_decode)\n    if preds.dtype != np.int64 and preds.dtype != np.int32:\n        print(f\"WARNING: preds dtype is {preds.dtype}, converting to int64.\")\n        preds = preds.astype(np.int64)\n    if labels.dtype != np.int64 and labels.dtype != np.int32:\n        print(f\"WARNING: labels dtype is {labels.dtype}, converting to int64.\")\n        labels = labels.astype(np.int64)\n\n    print(f\"preds shape (after processing): {preds.shape}\")\n    print(f\"labels shape (after processing): {labels.shape}\")\n\n    # Replace -100 in labels as they are ignored in loss calculation\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    print(\"Labels processed (replaced -100).\")\n    # --- End of FIX for Step 10 Decoding Error ---\n\n    # Decode predictions and labels back to text\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    print(f\"Successfully decoded {len(decoded_preds)} predictions and {len(decoded_labels)} labels.\")\n\n    # FIX: Replace empty strings with a single character 'a' to prevent ZeroDivisionError in BLEU\n    decoded_preds_for_metrics = [pred if pred else \"a\" for pred in decoded_preds]\n    decoded_labels_for_metrics = [label if label else \"a\" for label in decoded_labels]\n    print(\"Empty strings in decoded predictions/labels replaced with 'a' for robust metric calculation.\")\n\n\n    # Some metrics expect a list of lists for references\n    formated_decoded_labels_for_some_metrics = [[label] for label in decoded_labels_for_metrics]\n    print(\"Labels formatted for metrics.\")\n\n    # --- Compute Metrics ---\n    # Re-compute ROUGE and BLEU for the example summaries\n    try:\n        rouge_results_examples = metric_rouge.compute(predictions=decoded_preds_for_metrics, references=decoded_labels_for_metrics)\n        print(\"ROUGE computed for example summaries.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to compute ROUGE for example summaries: {e}\")\n        import traceback\n        traceback.print_exc()\n        rouge_results_examples = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0} # Fallback\n\n    print(\"\\nExample Summaries with ROUGE Scores (Adapters):\")\n    num_examples_to_display = 5 # Display a few examples\n    for i in range(min(num_examples_to_display, len(tokenized_test_dataset))):\n        original_text = dataset['test'][i]['text']\n        reference_summary = decoded_labels[i]\n        generated_summary = decoded_preds[i]\n\n        # Compute ROUGE for individual example (optional, but good for inspection)\n        single_rouge = metric_rouge.compute(predictions=[generated_summary if generated_summary else \"a\"],\n                                            references=[reference_summary if reference_summary else \"a\"])\n\n        print(f\"\\n--- Example {i+1} ---\")\n        print(f\"Original Text: {original_text[:500]}...\") # Truncate long texts for display\n        print(f\"Reference Summary: {reference_summary}\")\n        print(f\"Generated Summary: {generated_summary}\")\n        print(f\"  ROUGE-1 F1: {single_rouge['rouge1']:.4f}\")\n        print(f\"  ROUGE-2 F1: {single_rouge['rouge2']:.4f}\")\n        print(f\"  ROUGE-L F1: {single_rouge['rougeL']:.4f}\")\n\nexcept Exception as e:\n    print(f\"\\nERROR during Step 10 (Example Summaries Generation): {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\nScript finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T09:47:02.962072Z","iopub.execute_input":"2025-05-22T09:47:02.962336Z","iopub.status.idle":"2025-05-22T10:07:41.989145Z","shell.execute_reply.started":"2025-05-22T09:47:02.962314Z","shell.execute_reply":"2025-05-22T10:07:41.987830Z"}},"outputs":[{"name":"stdout","text":"Attempting to load trimmed model and tokenizer from: /kaggle/input/dataset1\nTraining outputs (checkpoints, logs, results) will be saved to: /kaggle/input/dataset1\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting transformers\n  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.52.2-py3-none-any.whl (10.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\nSuccessfully installed transformers-4.52.2\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\nCollecting adapters\n  Downloading adapters-1.2.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting rawpy\n  Downloading rawpy-0.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\nCollecting transformers~=4.51.3 (from adapters)\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from adapters) (25.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.72.0rc1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.51.3->adapters) (0.21.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading adapters-1.2.0-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rawpy-0.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=2f99a8558ed01458b0ab9689f3ebbdec119474ef5d526b00780aafba8516f774\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, rouge_score, rawpy, evaluate, adapters\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.2\n    Uninstalling transformers-4.52.2:\n      Successfully uninstalled transformers-4.52.2\nSuccessfully installed adapters-1.2.0 evaluate-0.4.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rawpy-0.25.0 rouge_score-0.1.2 transformers-4.51.3\nTransformers version: 4.51.3\n","output_type":"stream"},{"name":"stderr","text":"2025-05-22 09:49:02.083566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747907342.287037      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747907342.350022      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nLoading trimmed model from /kaggle/input/dataset1...\nLoading trimmed tokenizer from /kaggle/input/dataset1...\nTrimmed model and tokenizer loaded successfully!\nLoaded model vocabulary size: 2766\nLoaded tokenizer vocabulary size: 2866\nAdapters initialized for the trimmed model.\nModel moved to device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ceba7e5b144ed78593056795ae676b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce0f5cf4f1a4eb7bcf9c95ce98a4c9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59281e7487094630a770564306cc8bb8"}},"metadata":{}},{"name":"stdout","text":"\nDataset loaded from CSV files.\nDataset sizes:\nTrain size: 23492\nValidation size: 2937\nTest size: 2937\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23492 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca82f12d48c749988277b7c95f3e1237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2937 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4ed378804e4da2ab17887653fb8ef6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2937 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e1b140d05ed49108bcfa67cb84074ca"}},"metadata":{}},{"name":"stdout","text":"\nConfiguring and applying Adapters...\n\nTrainable parameters (Adapters on Trimmed Model):\n\nTrainable parameters (Adapter baseline):\nTotal parameters: 47960448\nTrainable parameters: 1065984\nPercentage of trainable parameters: 2.22%\nTraining outputs (checkpoints, logs) will be saved to: /kaggle/working/trimmed_adapters_checkpoints\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55c5b2ac92ee433ea2f715c646cd30d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8444c35268c54bb390f6ca283dde779d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74734aa8200a41628d919a3164ec15d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0da1c24709f2432fac0951e2527f265c"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/1405709105.py:421: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FilteredArgsSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = FilteredArgsSeq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training (Adapters on Trimmed Model)...\nNo checkpoint found, starting fresh training.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1001' max='8808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1001/8808 13:16 < 1:43:47, 1.25 it/s, Epoch 0.34/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>26.213300</td>\n      <td>4.493471</td>\n      <td>0.008387</td>\n      <td>0.001021</td>\n      <td>0.008240</td>\n      <td>0.001150</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='1367' max='1469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1367/1469 04:44 < 00:21, 4.80 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nCUDA cache cleared and gc.collect() called by callback before evaluation at step 500.\n\n--- Inside compute_metrics (DEBUG) ---\nType of eval_pred.predictions (initial): <class 'tuple'>\nType of eval_pred.label_ids (initial): <class 'numpy.ndarray'>\neval_pred.predictions is a tuple. Length: 2\nTaking first element of predictions tuple. New type: <class 'numpy.ndarray'>\nPredictions type (after initial processing): <class 'numpy.ndarray'>\nLabels type (after initial processing): <class 'numpy.ndarray'>\nWARNING: Predictions has unexpected 3 dimensions. Assuming logits and applying argmax.\nPredictions after argmax: New shape: (2937, 107), New dtype: int64\nPredictions shape (after processing): (2937, 107)\nLabels shape (after processing): (2937, 107)\nLabels processed (replaced -100).\nSuccessfully decoded 2937 predictions and 2937 labels.\nEmpty strings in decoded predictions/labels replaced with 'a' for robust metric calculation.\nLabels formatted for metrics.\nROUGE computed.\nBLEU computed.\nMetrics combined.\n--- Exiting compute_metrics ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nCUDA cache cleared and gc.collect() called by callback before evaluation at step 1000.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1405709105.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;31m# Start training (pass resume_from_checkpoint if a checkpoint was found)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2628\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3094\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3045\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     def predict(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4155\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4373\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_eval_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4375\u001b[0;31m                     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         return type(tensors)(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Now let's fill the result tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.66 GiB is free. Process 2549 has 12.08 GiB memory in use. Of the allocated memory 8.92 GiB is allocated by PyTorch, and 2.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.66 GiB is free. Process 2549 has 12.08 GiB memory in use. Of the allocated memory 8.92 GiB is allocated by PyTorch, and 2.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}