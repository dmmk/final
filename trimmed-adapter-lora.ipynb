{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11920450,"sourceType":"datasetVersion","datasetId":7494180}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"Trimmed mT5-small + LoRA + Adapters Fine-tuning on GPU with Google Drive Storage\"\"\"\n\n\nimport os # Import the os module\n\n# Define the base folder in Google Drive for data and outputs\n# This should be the folder containing your CSV data\nbase_drive_folder = \"/kaggle/input/dataset3\" # Keep this for data and general outputs\nbase_drive_folder_output = \"/kaggle/working/\" # Keep this for data and general outputs\n\n# Define the directory where the trimmed model and tokenizer were saved by vocabtrimmer\n# IMPORTANT: Update this path if you saved the trimmed model to a different location\ntrimmed_model_path = \"/kaggle/input/dataset3\"\nprint(f\"Attempting to load trimmed model and tokenizer from: {trimmed_model_path}\")\n\n# Define a specific output directory for this combined experiment\ntraining_output_dir = os.path.join(base_drive_folder_output, \"trimmed_lora_adapters_checkpoints\") # NEW output directory name for combined\n# Create the base output folder if it doesn't exist\nos.makedirs(training_output_dir, exist_ok=True)\nprint(f\"Training outputs (checkpoints, logs, results) will be saved to: {training_output_dir}\")\n\n\n# Step 1: Setup and Installation\n# Install necessary libraries\n# Added peft for LoRA, keeping adapters\n!pip install --upgrade transformers\n!pip istall -upgrade datasets\n!pip install adapters peft accelerate evaluate rouge_score nltk pandas rawpy Pillow tensorboard\n\n# Print transformers version to verify installation\nimport transformers\nprint(f\"Transformers version: {transformers.__version__}\")\n\n# Note: Meteor also requires Java to be installed on your system.\n# If you still get errors for Meteor, you might need to install Java.\n# Restart your Colab runtime after installation if prompted.\n\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport numpy as np\nimport gc # Import garbage collector\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, TrainerCallback\n# Import PEFT library components\nfrom peft import LoraConfig, get_peft_model, TaskType, PeftModel # Import PeftModel for loading\n# Import Adapter-Hub library and AdapterConfig\nfrom adapters import AdapterConfig\nimport adapters # Import the adapters library to patch AutoModel classes\n\nfrom datasets import load_dataset, DatasetDict, Dataset\n\nimport evaluate\nimport nltk\n\n# Ensure necessary nltk data is downloaded for metrics\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"wordnet\", quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger', quiet=True) # sometimes needed for meteor, but keeping for now as it's small\n\n# Step 2: Load the Trimmed Model and Tokenizer\n# Load the model and tokenizer from the directory where vocabtrimmer saved them\ntry:\n    print(f\"\\nLoading trimmed model from {trimmed_model_path}...\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(trimmed_model_path)\n    print(f\"Loading trimmed tokenizer from {trimmed_model_path}...\")\n    tokenizer = AutoTokenizer.from_pretrained(trimmed_model_path)\n    print(\"Trimmed model and tokenizer loaded successfully!\")\n    print(f\"Loaded model vocabulary size: {model.config.vocab_size}\")\n    print(f\"Loaded tokenizer vocabulary size: {len(tokenizer)}\")\n\nexcept Exception as e:\n    print(f\"Error loading trimmed model or tokenizer from {trimmed_model_path}: {e}\")\n    print(\"Please ensure the trimmed model and tokenizer were saved correctly by vocabtrimmer\")\n    print(\"and that the 'trimmed_model_path' variable points to the correct directory.\")\n    print(\"Exiting script.\")\n    raise SystemExit(\"Failed to load trimmed model/tokenizer.\")\n\n\n# Check for CUDA availability and move model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"Base model moved to device: {device}\")\n\n\n# Step 3: Load and Preprocess Your Data (Using three CSV files from Google Drive)\n\n# Define the paths to your CSV files within the Google Drive folder\ntrain_csv_path = os.path.join(base_drive_folder, 'amharic_3_train.csv')\nvalid_csv_path = os.path.join(base_drive_folder, 'amharic_3_valid.csv')\ntest_csv_path = os.path.join(base_drive_folder, 'amharic_3_test.csv')\n\n# IMPORTANT: Make sure you have uploaded amharic_3_train.csv, amharic_3_valid.csv,\n# and amharic_3_test.csv into the specified base_drive_folder in your Google Drive.\n\n# Load each dataset split from its respective CSV file\ntry:\n    # load_dataset returns a DatasetDict, get the 'train' split for each file\n    train_dataset = load_dataset('csv', data_files=train_csv_path)['train']\n    valid_dataset = load_dataset('csv', data_files=valid_csv_path)['train']\n    test_dataset = load_dataset('csv', data_files=test_csv_path)['train']\n\n    # Select only the 'text' and 'summary' columns for each dataset\n    train_dataset = train_dataset.select_columns(['text', 'summary'])\n    valid_dataset = valid_dataset.select_columns(['text', 'summary'])\n    test_dataset = test_dataset.select_columns(['text', 'summary'])\n\n    # Combine into a DatasetDict\n    dataset = DatasetDict({\n        'train': train_dataset,\n        'validation': valid_dataset,\n        'test': test_dataset\n    })\n    print(\"\\nDataset loaded from CSV files.\")\n\nexcept FileNotFoundError:\n    print(\"\\nError: One or more of the specified CSV files were not found in Google Drive.\")\n    print(f\"Please make sure amharic_3_train.csv, amharic_3_valid.csv, and amharic_3_test.csv\")\n    print(f\"are uploaded to your Google Drive folder: {base_drive_folder}\")\n    # Create dummy datasets for demonstration if files not found\n    print(\"Creating dummy datasets for demonstration.\")\n    dummy_train_data = {\"text\": [\"Train document one for dummy data.\", \"Train document two for dummy data.\"], \"summary\": [\"Train sum 1.\", \"Train sum 2.\"]}\n    dummy_valid_data = {\"text\": [\"Valid document one for dummy data.\"], \"summary\": [\"Valid sum 1.\"]}\n    dummy_test_data = {\"text\": [\"Test document one for dummy data.\"], \"summary\": [\"Test sum 1.\"]}\n    dataset = DatasetDict({\n        'train': Dataset.from_dict(dummy_train_data),\n        'validation': Dataset.from_dict(dummy_valid_data),\n        'test': Dataset.from_dict(dummy_test_data)\n    })\n\n\nprint(\"Dataset sizes:\")\nprint(f\"Train size: {len(dataset['train'])}\")\nprint(f\"Validation size: {len(dataset['validation'])}\")\nprint(f\"Test size: {len(dataset['test'])}\")\n\n# Define the preprocessing function (DataCollator will handle padding)\n# Use the loaded (trimmed) tokenizer\ndef preprocess_function(examples):\n    # Tokenize documents (text column)\n    # Adjust max_length based on the typical length of your documents\n    model_inputs = tokenizer(\n        examples['text'],\n        max_length=512, # Max length for inputs\n        truncation=True,\n        # padding is handled by DataCollatorForSeq2Seq\n    )\n\n    # Tokenize summaries (summary column)\n    # Adjust max_length based on the typical length of your summaries\n    labels = tokenizer(\n        examples['summary'],\n        max_length=128, # Max length for labels\n        truncation=True,\n        # padding is handled by DataCollatorForSeq2Seq\n    )\n\n    # Assign input_ids of summaries as labels\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n    return model_inputs\n\n# Apply the preprocessing function to all splits\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\n\n# Set the format of the datasets to PyTorch tensors\n# Specify which columns should be converted to tensors\ntokenized_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n\n# Now, the variables for each split are directly available\ntokenized_train_dataset = tokenized_dataset['train']\ntokenized_eval_dataset = tokenized_dataset['validation']\ntokenized_test_dataset = tokenized_dataset['test']\n\n\n# Step 4: Configure and Apply LoRA and Adapters (REVISED ORDER)\n\nprint(\"\\nConfiguring and applying LoRA and Adapters...\")\n\n# 4a: Manually freeze all base model parameters\n# This is crucial so that only LoRA and Adapter parameters are trained\nfor param in model.parameters():\n    param.requires_grad = False\nprint(\"Base model parameters explicitly frozen.\")\n\n# 4b: Apply Adapters FIRST (on the base model)\nadapter_name = \"amharic_summarization_adapter\"\nadapter_config = AdapterConfig.load(\"double_seq_bn\") # Houlsby adapter type\n\n# Initialize adapters for the model (which is currently an AutoModelForSeq2SeqLM)\nadapters.init(model)\nprint(\"Adapters initialized for the base model.\")\n\n# Add the adapter to the model using the loaded configuration object\n# This call now happens on the AutoModelForSeq2SeqLM instance, which is correctly patched\nmodel.add_adapter(adapter_name, config=adapter_config)\nprint(f\"Adapter '{adapter_name}' added to the model.\")\n\n# Set the active adapter. This makes the adapter layers trainable.\nmodel.set_active_adapters(adapter_name)\nprint(f\"Adapter '{adapter_name}' set as active and its parameters are trainable.\")\n\n\n# 4c: Apply LoRA AFTER Adapters (on the adapter-enabled base model)\nlora_config = LoraConfig(\n    r=8, # LoRA attention dimension - Adjust based on experimentation\n    lora_alpha=16, # Alpha parameter for LoRA scaling - Adjust based on experimentation\n    target_modules=[\"q\", \"v\"], # Common target modules for attention in T5\n    lora_dropout=0.1, # Dropout probability for LoRA layers\n    bias=\"none\", # Bias type: 'none', 'all', or 'lora_only'\n    task_type=TaskType.SEQ_2_SEQ_LM # Task type for sequence-to-sequence models\n)\n# Apply LoRA to the (now frozen and adapter-enabled) base model.\n# This will wrap the model and make LoRA layers trainable.\nmodel = get_peft_model(model, lora_config)\nprint(\"LoRA layers added to the adapter-enabled model.\")\n\n\n# Print trainable parameters (will now show LoRA and adapter parameters)\nprint(\"\\nTrainable parameters (LoRA + Adapters):\")\n# Calculate total trainable parameters using a generator expression\ntotal_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n# Calculate total parameters (optional, for context)\ntotal_all_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"Total parameters: {total_all_params}\")\nprint(f\"Trainable parameters: {total_trainable_params}\")\n# Calculate and print the percentage of trainable parameters\nif total_all_params > 0:\n    print(f\"Percentage of trainable parameters: {100 * total_trainable_params / total_all_params:.2f}%\")\nelse:\n    print(\"Percentage of trainable parameters: 0.00%\")\n\n\n# --- Custom Callback for Cache Clearing ---\nclass ClearCacheCallback(TrainerCallback):\n    \"\"\"\n    A TrainerCallback that clears the CUDA cache before evaluation steps.\n    Using on_step_end hook to check if evaluation is about to happen.\n    Includes gc.collect().\n    \"\"\"\n    def on_step_end(self, args, state, control, **kwargs):\n        # Check if evaluation is scheduled for the next step\n        # This condition is true when state.global_step is a multiple of args.eval_steps\n        # and state.global_step > 0\n        if args.eval_strategy == \"steps\" and state.global_step > 0 and args.eval_steps and state.global_step % args.eval_steps == 0:\n             if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                # Also try garbage collection\n                gc.collect()\n                print(f\"\\nCUDA cache cleared and gc.collect() called by callback before evaluation at step {state.global_step}.\")\n                # Set control.should_evaluate to True to ensure evaluation happens right after this step end\n                control.should_evaluate = True\n\n# --- Custom Trainer to filter unexpected arguments (REVISED AGAIN for compute_loss signature and return order) ---\nfrom typing import Dict, Any, Union, Optional\nimport torch.nn as nn # Import for type hinting\nfrom transformers.modeling_outputs import Seq2SeqLMOutput # Import for type hinting\n\nclass FilteredArgsSeq2SeqTrainer(Seq2SeqTrainer):\n    \"\"\"\n    A custom Seq2SeqTrainer that filters out the 'num_items_in_batch' argument\n    before passing inputs to the model's forward method, as it can cause TypeErrors.\n    The compute_loss signature is also updated to accept 'num_items_in_batch'\n    and its return order is corrected when return_outputs=True.\n    \"\"\"\n    def compute_loss(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        \"\"\"\n        Compute the loss for the model. Filters out 'num_items_in_batch' from inputs.\n        Corrects return order when return_outputs=True.\n        \"\"\"\n        # Create a copy of inputs to modify, preventing modification of the original dict\n        modified_inputs = inputs.copy()\n\n        # Explicitly remove 'num_items_in_batch' if it exists in the inputs dictionary\n        # This is crucial because the model's forward method does not expect it.\n        if 'num_items_in_batch' in modified_inputs:\n            del modified_inputs['num_items_in_batch']\n\n        # Now call the original compute_loss logic with the filtered inputs\n        if self.label_smoother is not None and \"labels\" in modified_inputs:\n            labels = modified_inputs.pop(\"labels\")\n        else:\n            labels = None\n\n        outputs = model(**modified_inputs) # This is where the filtered inputs are used\n\n        # Save past state if it exists\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            if self.label_smoother is not None:\n                loss = self.label_smoother(outputs, labels)\n            else:\n                # We don't use outputs.loss directly because it might not be present\n                # if the model is in eval mode or if the loss is computed externally.\n                # In training, outputs[0] is typically the loss.\n                loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0])\n        else:\n            if isinstance(outputs, dict) and \"loss\" in outputs:\n                loss = outputs[\"loss\"]\n            else:\n                loss = outputs[0] # Fallback if 'loss' key is not present but outputs is a tuple/list\n\n        if return_outputs:\n            # CRITICAL FIX: Return (loss, outputs) as expected by Trainer's prediction_step\n            return (loss, outputs)\n        else:\n            return loss\n\n\n# Step 5: Define Training Arguments and Trainer (Configured for GPU with Drive Outputs)\n\n# Instantiate Data Collator (Handles padding batches dynamically)\n# We pass the tokenizer so it knows the pad token ID\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model) # Pass the combined model here\n\n# Define training arguments (Revised for GPU with Drive Checkpointing and TensorBoard)\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=training_output_dir, # Output directory for checkpoints and logs (in Drive)\n    num_train_epochs=3, # Number of training epochs - Adjust based on convergence\n    per_device_train_batch_size=2, # Batch size per device during training\n    per_device_eval_batch_size=1, # Evaluation batch size\n    gradient_accumulation_steps=2, # Gradient accumulation steps to compensate for smaller batch size\n    learning_rate=1e-5, # Keep reduced learning rate for stability\n    weight_decay=0.01, # Weight decay - Adjust based on experimentation\n    eval_strategy=\"steps\", # Evaluate every N steps to match save_strategy\n    eval_steps=500, # Evaluate every 500 training steps (matches save_steps)\n    save_strategy=\"steps\", # Save checkpoint every N steps\n    save_steps=500, # Save a checkpoint every 500 training steps\n    save_total_limit=3, # Optional: Limit the total number of checkpoints to save\n    load_best_model_at_end=True, # Load the best model at the end of training based on metric_for_best_model\n    metric_for_best_model=\"rougeL\", # Metric to monitor for best model - Change if needed\n    greater_is_better=True, # Set to False for metrics like loss\n    report_to=\"tensorboard\", # Log metrics to TensorBoard\n    push_to_hub=False, # Set to True if you want to push to Hugging Face Hub\n    label_names=[\"labels\"], # Explicitly tell the Trainer your label column name\n    fp16=False, # Keep mixed precision training disabled for stability\n)\n\n# Define compute_metrics function for evaluation\nmetric_rouge = evaluate.load(\"rouge\")\nmetric_bleu = evaluate.load(\"bleu\")\n\ndef compute_metrics(eval_pred):\n    predictions = eval_pred.predictions\n    labels = eval_pred.label_ids\n\n    # Debugging prints to inspect types and shapes\n    print(\"\\n--- Inside compute_metrics (DEBUG) ---\")\n    print(f\"Type of eval_pred.predictions (initial): {type(eval_pred.predictions)}\")\n    print(f\"Type of eval_pred.label_ids (initial): {type(eval_pred.label_ids)}\")\n\n    # CRITICAL FIX: If predictions is a tuple (e.g., logits and hidden states), take the first element (logits)\n    if isinstance(predictions, tuple):\n        print(f\"eval_pred.predictions is a tuple. Length: {len(predictions)}\")\n        predictions = predictions[0] # Assume the first element is the actual generated sequences (logits)\n        print(f\"Taking first element of predictions tuple. New type: {type(predictions)}\")\n\n    # Ensure predictions and labels are NumPy arrays\n    if isinstance(predictions, torch.Tensor):\n        predictions = predictions.cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.cpu().numpy()\n\n    print(f\"Predictions type (after initial processing): {type(predictions)}\")\n    print(f\"Labels type (after initial processing): {type(labels)}\")\n\n    # CRITICAL FIX: If predictions are logits (3D array), convert to token IDs using argmax\n    if predictions.ndim == 3:\n        print(f\"WARNING: Predictions has unexpected 3 dimensions. Assuming logits and applying argmax.\")\n        predictions = np.argmax(predictions, axis=-1) # Convert logits to token IDs\n        print(f\"Predictions after argmax: New shape: {predictions.shape}, New dtype: {predictions.dtype}\")\n\n    # Further check for dimensions: if predictions is 3D (e.g., batch_size, seq_len, 1), squeeze it\n    # This handles cases where the model output might have an extra dimension of size 1\n    if predictions.ndim == 3 and predictions.shape[-1] == 1:\n        predictions = predictions.squeeze(-1)\n        print(f\"Predictions was 3D with last dim 1, squeezed to 2D. New shape: {predictions.shape}\")\n    elif predictions.ndim > 2:\n        # If it's more than 2D and not just a trailing 1, this is unexpected for token IDs\n        print(f\"WARNING: Predictions has unexpected {predictions.ndim} dimensions (after argmax check). Expected 2D.\")\n        predictions = predictions.reshape(predictions.shape[0], -1) # Attempt to flatten if possible\n        print(f\"Attempted to reshape predictions to 2D. New shape: {predictions.shape}\")\n\n    # Ensure predictions are integers (important for tokenizer.batch_decode)\n    if predictions.dtype != np.int64 and predictions.dtype != np.int32:\n        print(f\"WARNING: Predictions dtype is {predictions.dtype}, converting to int64.\")\n        predictions = predictions.astype(np.int64)\n    if labels.dtype != np.int64 and labels.dtype != np.int32:\n        print(f\"WARNING: Labels dtype is {labels.dtype}, converting to int64.\")\n        labels = labels.astype(np.int64)\n\n    print(f\"Predictions shape (after processing): {predictions.shape}\")\n    print(f\"Labels shape (after processing): {labels.shape}\")\n\n    # Replace -100 in labels as they are ignored in loss calculation\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    print(\"Labels processed (replaced -100).\")\n\n    try:\n        # Decode predictions and labels, removing special tokens\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        print(f\"Successfully decoded {len(decoded_preds)} predictions and {len(decoded_labels)} labels.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to decode predictions/labels in compute_metrics: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise # Re-raise to crash session with specific error\n\n    # FIX: Replace empty strings with a single character 'a' to prevent ZeroDivisionError in BLEU\n    # This is crucial for metrics like BLEU that perform length calculations.\n    decoded_preds_for_metrics = [pred if pred else \"a\" for pred in decoded_preds]\n    decoded_labels_for_metrics = [label if label else \"a\" for label in decoded_labels]\n    print(\"Empty strings in decoded predictions/labels replaced with 'a' for robust metric calculation.\")\n\n    # Some metrics expect a list of lists for references\n    formated_decoded_labels_for_some_metrics = [[label] for label in decoded_labels_for_metrics] # Use the cleaned lists for metrics\n    print(\"Labels formatted for metrics.\")\n\n    # --- Compute Metrics ---\n    try:\n        # Rouge\n        rouge_results = metric_rouge.compute(predictions=decoded_preds_for_metrics, references=decoded_labels_for_metrics)\n        print(\"ROUGE computed.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to compute ROUGE in compute_metrics: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise # Re-raise\n\n    try:\n        # BLEU (requires references as list of lists)\n        bleu_results = metric_bleu.compute(predictions=decoded_preds_for_metrics, references=formated_decoded_labels_for_some_metrics)\n        print(\"BLEU computed.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to compute BLEU in compute_metrics: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise # Re-raise\n\n    # Combine metrics - only include ROUGE and BLEU\n    combined_results = {\n        \"rouge1\": rouge_results[\"rouge1\"], # F1 score is commonly reported\n        \"rouge2\": rouge_results[\"rouge2\"],\n        \"rougeL\": rouge_results[\"rougeL\"],\n        \"bleu\": bleu_results[\"bleu\"],\n    }\n    print(\"Metrics combined.\")\n    print(\"--- Exiting compute_metrics ---\")\n    # The Trainer expects a dictionary of metrics\n    return combined_results\n\n\n# Create Trainer instance, adding the custom callback\ntrainer = FilteredArgsSeq2SeqTrainer(\n    model=model, # This is the combined LoRA + Adapter model on trimmed base\n    args=training_args, # These are the GPU-configured args with Drive checkpointing and reporting\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_eval_dataset,\n    tokenizer=tokenizer, # Pass the trimmed tokenizer here\n    data_collator=data_collator, # Add the data collator here\n    compute_metrics=compute_metrics, # Pass the compute_metrics function\n    callbacks=[ClearCacheCallback()], # Add the custom cache clearing callback here\n)\n\n\n# Step 6: Train the Model & Measure Time (Includes Resuming from Checkpoint in Drive)\nprint(\"\\nStarting training (LoRA + Adapters on Trimmed Model)...\")\n\n# --- Resume Training Logic ---\n# Check if there's a checkpoint to resume from in the output directory\n# The Trainer saves checkpoints in subdirectories like 'checkpoint-XXXX'\nlatest_checkpoint_dir = None\nif os.path.isdir(training_args.output_dir):\n    # Find all checkpoint directories\n    checkpoints = [m for m in os.listdir(training_args.output_dir) if m.startswith('checkpoint-')]\n    if checkpoints:\n        # Sort checkpoints by step number to find the latest\n        latest_checkpoint_dir = os.path.join(training_args.output_dir, sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1])\n        print(f\"Found latest checkpoint: {latest_checkpoint_dir}\")\n\n# Set resume_from_checkpoint to the latest found checkpoint if it exists\nresume_from_checkpoint = latest_checkpoint_dir if latest_checkpoint_dir and os.path.isdir(latest_checkpoint_dir) else None\n\nif resume_from_checkpoint:\n     print(f\"Resuming training from checkpoint: {resume_from_checkpoint}\")\nelse:\n     print(\"No checkpoint found, starting fresh training.\")\n\nstart_time = time.time()\n\n# Start training (pass resume_from_checkpoint if a checkpoint was found)\ntrain_output = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\nend_time = time.time()\ntotal_training_time_seconds = end_time - start_time\ntotal_training_time_minutes = total_training_time_seconds / 60.0\n\nprint(f\"\\nTotal training time (LoRA + Adapters on Trimmed Model): {total_training_time_seconds:.2f} seconds ({total_training_time_minutes:.2f} minutes)\")\n\n\n# Step 7: Evaluate the Model on the Test Set and Get Metrics\nprint(\"\\n--- Starting Model Evaluation on Test Set ---\")\ntry:\n    # --- Explicitly Clear CUDA Cache Before Final Evaluation ---\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect() # Also call gc.collect() here\n        print(\"\\nCUDA cache cleared and gc.collect() called explicitly before final evaluation.\")\n    else:\n        print(\"\\nCUDA not available, skipping cache clear before final evaluation.\")\n\n    print(\"Calling trainer.evaluate...\")\n    evaluation_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n    print(\"trainer.evaluate completed.\")\n\n    print(\"\\nEvaluation Metrics (LoRA + Adapters on Trimmed Model):\")\n    print(evaluation_results)\n\n    # --- Save Evaluation Results to File ---\n    eval_results_file = os.path.join(base_drive_folder_output, \"trimmed_lora_adapters_evaluation_results.json\") # NEW file name\n    with open(eval_results_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(evaluation_results, f, indent=4)\n    print(f\"\\nEvaluation results saved to: {eval_results_file}\")\n\nexcept Exception as e:\n    print(f\"\\nERROR during Step 7 (Model Evaluation): {e}\")\n    import traceback\n    traceback.print_exc() # Print full traceback for detailed error\n\n\n# Step 8: Plot Training vs Validation Loss for Convergence Analysis\nprint(\"\\n--- Starting Loss Curve Plotting ---\")\ntry:\n    # Extract loss and eval_loss from the trainer's log history\n    train_loss_entries = [entry for entry in trainer.state.log_history if 'loss' in entry]\n    eval_loss_entries = [entry for entry in trainer.state.log_history if 'eval_loss' in entry]\n\n    train_steps = [entry['step'] for entry in train_loss_entries]\n    train_losses = [entry['loss'] for entry in train_loss_entries]\n\n    # For evaluation, the step corresponds to the global step at which evaluation was performed\n    eval_steps = [entry['step'] for entry in eval_loss_entries]\n    eval_losses = [entry['eval_loss'] for entry in eval_loss_entries]\n\n    if train_steps and eval_steps:\n        plt.figure(figsize=(10, 6))\n        plt.plot(train_steps, train_losses, label=\"Training Loss\", marker='o', linestyle='-')\n        plt.plot(eval_steps, eval_losses, label=\"Validation Loss\", marker='s', linestyle='--')\n        plt.xlabel(\"Training Steps\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Training vs Validation Loss Over Steps (LoRA + Adapters)\") # NEW plot title\n        plt.legend()\n        plt.grid(True)\n\n        # Save the plot to Google Drive\n        loss_plot_path = os.path.join(base_drive_folder_output, \"trimmed_lora_adapters_loss_curve.png\") # NEW file name\n        plt.savefig(loss_plot_path)\n        print(f\"Loss curve plot saved to: {loss_plot_path}\")\n        plt.show()\n    else:\n        print(\"Not enough data in log history to plot loss curves. Ensure eval_strategy and logging_steps are configured.\")\nexcept Exception as e:\n    print(f\"\\nERROR during Step 8 (Loss Plotting): {e}\")\n    import traceback\n    traceback.print_exc()\n\n\n# Step 9: Save the Final (Best) LoRA Weights and Adapter Weights to Drive\nprint(\"\\n--- Starting Saving LoRA and Adapter Weights ---\")\ntry:\n    # After trainer.train() finishes (and because load_best_model_at_end=True),\n    # the model object in memory is the best model found during training.\n\n    # Save LoRA weights (since 'model' is a PeftModel, save_pretrained saves LoRA weights)\n    lora_output_dir = os.path.join(base_drive_folder_output, \"trimmed_lora_adapters_final_best_lora\") # NEW output directory name\n    model.save_pretrained(lora_output_dir)\n    print(f\"\\nFinal (best) LoRA weights saved to {lora_output_dir}\")\n\n    # Save Adapter weights (using the adapters library's method)\n    # The adapter is part of the 'model' object, so we save it from there.\n    adapters_output_dir = os.path.join(base_drive_folder_output, \"trimmed_lora_adapters_final_best_adapter\") # NEW output directory name\n    # When saving from a PeftModel, you need to access the underlying model to save the adapter\n    # The PeftModel itself doesn't directly expose model.save_adapter.\n    # However, the adapters library patches the base model, so the methods should be available on the underlying model.\n    # Let's try saving the adapter directly from the model object, which is now a PeftModel.\n    # If this fails, we might need to access model.base_model\n    model.save_adapter(adapters_output_dir, adapter_name) # Save only the specified adapter\n    print(f\"Final (best) adapter weights for '{adapter_name}' saved to {adapters_output_dir}\")\n\n    # To save the tokenizer along with the weights (recommended):\n    tokenizer.save_pretrained(lora_output_dir) # Save with LoRA weights\n    tokenizer.save_pretrained(adapters_output_dir) # Also save with adapter weights (redundant but safe)\n    print(f\"Tokenizer saved to {lora_output_dir} and {adapters_output_dir}\")\n\n\n    # --- How to Load the Saved Trimmed Model with LoRA and Adapter Weights Later ---\n    print(\"\\n--- How to Load the Saved Trimmed Model with LoRA and Adapter Weights Later ---\")\n    print(f\"To load the trimmed base model and apply the saved LoRA and adapter weights later, you would do:\")\n    print(f\"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\")\n    print(f\"from peft import PeftModel, LoraConfig, TaskType\")\n    print(f\"from adapters import AdapterConfig\")\n    print(f\"import adapters\")\n    print(f\"import torch\")\n    print(f\"\")\n    print(f\"# Path to the directory where you saved the trimmed base model (from vocabtrimmer)\")\n    print(f\"trimmed_base_model_path = '{trimmed_model_path}'\")\n    print(f\"# Path to the directory where you saved the fine-tuned LoRA weights\")\n    print(f\"saved_lora_dir = '{lora_output_dir}'\")\n    print(f\"# Path to the directory where you saved the fine-tuned adapter weights\")\n    print(f\"saved_adapters_dir = '{adapters_output_dir}'\")\n    print(f\"adapter_name = '{adapter_name}'\")\n    print(f\"\")\n    print(f\"# Load the trimmed base model\")\n    print(f\"loaded_base_model = AutoModelForSeq2SeqLM.from_pretrained(trimmed_base_model_path)\")\n    print(f\"\")\n    print(f\"# Initialize adapters for the base model (before applying LoRA)\")\n    print(f\"adapters.init(loaded_base_model)\")\n    print(f\"\")\n    print(f\"# Load the adapter weights and add them to the base model\")\n    print(f\"# Use AdapterConfig.load() to get the correct config type\")\n    print(f\"loaded_base_model.load_adapter(saved_adapters_dir, config=AdapterConfig.load('double_seq_bn'), load_as=adapter_name)\")\n    print(f\"\")\n    print(f\"# Load the LoRA weights into the adapter-enabled base model (this returns a PeftModel)\")\n    print(f\"loaded_peft_model = PeftModel.from_pretrained(loaded_base_model, saved_lora_dir)\")\n    print(f\"\")\n    print(f\"# Set the adapter as active for inference on the PeftModel (optional, but good practice if you want to use adapter-specific features)\")\n    print(f\"loaded_peft_model.set_active_adapters(adapter_name)\")\n    print(f\"\")\n    print(f\"# Load the tokenizer (can be loaded from either saved LoRA or adapter dir)\")\n    print(f\"loaded_tokenizer = AutoTokenizer.from_pretrained(saved_lora_dir)\")\n    print(f\"\")\n    print(f\"# If running on GPU, ensure model is on GPU\")\n    print(f\"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\")\n    print(f\"loaded_peft_model.to(device)\")\n    print(f\"\")\n    print(f\"# Set to evaluation mode\")\n    print(f\"loaded_peft_model.eval()\")\n    print(f\"\")\n    print(f\"print(f'Successfully loaded trimmed model with LoRA and adapter weights from {saved_lora_dir} and {saved_adapters_dir}')\")\n    print(f\"print(f'Loaded model vocabulary size: {loaded_peft_model.config.vocab_size}')\")\n    print(f\"print(f'Loaded tokenizer vocabulary size: {len(loaded_tokenizer)}')\")\n    print(f\"\")\n    print(f\"# Example Inference (Optional)\")\n    print(f\"# text_to_summarize = 'የአማርኛ ጽሑፍ እዚህ ይገባል...'\")\n    print(f\"# inputs = loaded_tokenizer(text_to_summarize, return_tensors='pt', max_length=512, truncation=True).to(device)\")\n    print(f\"# output_tokens = loaded_peft_model.generate(**inputs, max_new_tokens=128, num_beams=4, early_stopping=True)\")\n    print(f\"# generated_summary = loaded_tokenizer.decode(output_tokens[0], skip_special_tokens=True)\")\n    print(f\"# print(f'Generated Summary: {generated_summary}')\")\n\nexcept Exception as e:\n    print(f\"\\nERROR during Step 9 (LoRA and Adapter Saving): {e}\")\n    import traceback\n    traceback.print_exc()\n\n\n# Step 10: Display Example Summaries with ROUGE Scores\nprint(\"\\n--- Starting Example Summaries Generation ---\")\ntry:\n    # Get predictions for the test set\n    generated_predictions = trainer.predict(\n        test_dataset=tokenized_test_dataset,\n        max_new_tokens=128, # Max length of generated summary (matches labels)\n        num_beams=4, # Number of beams for beam search\n    )\n    print(\"trainer.predict for example summaries completed.\")\n\n    preds = generated_predictions.predictions\n    labels = generated_predictions.label_ids\n\n    # --- Start of FIX for Step 10 Decoding Error ---\n    print(\"\\n--- Inside Step 10 Decoding (DEBUG) ---\")\n    print(f\"Type of preds (initial): {type(preds)}\")\n    print(f\"Type of labels (initial): {type(labels)}\")\n\n    # CRITICAL FIX: If predictions is a tuple (e.g., logits and hidden states), take the first element (logits)\n    if isinstance(preds, tuple):\n        print(f\"preds is a tuple. Length: {len(preds)}\")\n        preds = preds[0] # Assume the first element is the actual generated sequences (logits)\n        print(f\"Taking first element of preds tuple. New type: {type(preds)}\")\n\n    # Ensure predictions and labels are NumPy arrays\n    if isinstance(preds, torch.Tensor):\n        preds = preds.cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.cpu().numpy()\n\n    print(f\"preds type (after initial processing): {type(preds)}\")\n    print(f\"labels type (after initial processing): {type(labels)}\")\n\n    # CRITICAL FIX: If predictions are logits (3D array), convert to token IDs using argmax\n    if preds.ndim == 3:\n        print(f\"WARNING: preds has unexpected 3 dimensions. Assuming logits and applying argmax.\")\n        preds = np.argmax(preds, axis=-1) # Convert logits to token IDs\n        print(f\"preds after argmax: New shape: {preds.shape}, New dtype: {preds.dtype}\")\n\n    # Further check for dimensions: if preds is 3D (e.g., batch_size, seq_len, 1), squeeze it\n    if preds.ndim == 3 and preds.shape[-1] == 1:\n        preds = preds.squeeze(-1)\n        print(f\"preds was 3D with last dim 1, squeezed to 2D. New shape: {preds.shape}\")\n    elif preds.ndim > 2:\n        print(f\"WARNING: preds has unexpected {preds.ndim} dimensions (after argmax check). Expected 2D.\")\n        preds = preds.reshape(preds.shape[0], -1) # Attempt to flatten if possible\n        print(f\"Attempted to reshape preds to 2D. New shape: {preds.shape}\")\n\n    # Ensure predictions are integers (important for tokenizer.batch_decode)\n    if preds.dtype != np.int64 and preds.dtype != np.int32:\n        print(f\"WARNING: preds dtype is {preds.dtype}, converting to int64.\")\n        preds = preds.astype(np.int64)\n    if labels.dtype != np.int64 and labels.dtype != np.int32:\n        print(f\"WARNING: labels dtype is {labels.dtype}, converting to int64.\")\n        labels = labels.astype(np.int64)\n\n    print(f\"preds shape (after processing): {preds.shape}\")\n    print(f\"labels shape (after processing): {labels.shape}\")\n\n    # Replace -100 in labels as they are ignored in loss calculation\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    print(\"Labels processed (replaced -100).\")\n    # --- End of FIX for Step 10 Decoding Error ---\n\n    # Decode predictions and labels back to text\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    print(f\"Successfully decoded {len(decoded_preds)} predictions and {len(decoded_labels)} labels.\")\n\n    # FIX: Replace empty strings with a single character 'a' to prevent ZeroDivisionError in BLEU\n    decoded_preds_for_metrics = [pred if pred else \"a\" for pred in decoded_preds]\n    decoded_labels_for_metrics = [label if label else \"a\" for label in decoded_labels]\n    print(\"Empty strings in decoded predictions/labels replaced with 'a' for robust metric calculation.\")\n\n\n    # Some metrics expect a list of lists for references\n    formated_decoded_labels_for_some_metrics = [[label] for label in decoded_labels_for_metrics]\n    print(\"Labels formatted for metrics.\")\n\n    # --- Compute Metrics ---\n    # Re-compute ROUGE and BLEU for the example summaries\n    try:\n        rouge_results_examples = metric_rouge.compute(predictions=decoded_preds_for_metrics, references=decoded_labels_for_metrics)\n        print(\"ROUGE computed for example summaries.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to compute ROUGE for example summaries: {e}\")\n        import traceback\n        traceback.print_exc()\n        rouge_results_examples = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0} # Fallback\n\n    print(\"\\nExample Summaries with ROUGE Scores (LoRA + Adapters):\")\n    num_examples_to_display = 5 # Display a few examples\n    for i in range(min(num_examples_to_display, len(tokenized_test_dataset))):\n        original_text = dataset['test'][i]['text']\n        reference_summary = decoded_labels[i]\n        generated_summary = decoded_preds[i]\n\n        # Compute ROUGE for individual example (optional, but good for inspection)\n        single_rouge = metric_rouge.compute(predictions=[generated_summary if generated_summary else \"a\"],\n                                            references=[reference_summary if reference_summary else \"a\"])\n\n        print(f\"\\n--- Example {i+1} ---\")\n        print(f\"Original Text: {original_text[:500]}...\") # Truncate long texts for display\n        print(f\"Reference Summary: {reference_summary}\")\n        print(f\"Generated Summary: {generated_summary}\")\n        print(f\"  ROUGE-1 F1: {single_rouge['rouge1']:.4f}\")\n        print(f\"  ROUGE-2 F1: {single_rouge['rouge2']:.4f}\")\n        print(f\"  ROUGE-L F1: {single_rouge['rougeL']:.4f}\")\n\nexcept Exception as e:\n    print(f\"\\nERROR during Step 10 (Example Summaries Generation): {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\nScript finished.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T15:31:36.266910Z","iopub.execute_input":"2025-05-23T15:31:36.267563Z","iopub.status.idle":"2025-05-23T15:53:01.436999Z","shell.execute_reply.started":"2025-05-23T15:31:36.267535Z","shell.execute_reply":"2025-05-23T15:53:01.435629Z"}},"outputs":[{"name":"stdout","text":"Attempting to load trimmed model and tokenizer from: /kaggle/input/dataset3\nTraining outputs (checkpoints, logs, results) will be saved to: /kaggle/working/trimmed_lora_adapters_checkpoints\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting transformers\n  Using cached transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nUsing cached transformers-4.52.3-py3-none-any.whl (10.5 MB)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nadapters 1.2.0 requires transformers~=4.51.3, but you have transformers 4.52.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed transformers-4.52.3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"ERROR: unknown command \"istall\" - maybe you meant \"install\"\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: adapters in /usr/local/lib/python3.11/dist-packages (1.2.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: rawpy in /usr/local/lib/python3.11/dist-packages (0.25.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\nCollecting transformers~=4.51.3 (from adapters)\n  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from adapters) (25.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.31.1)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.72.0rc1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.51.3->adapters) (0.21.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nUsing cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.3\n    Uninstalling transformers-4.52.3:\n      Successfully uninstalled transformers-4.52.3\nSuccessfully installed transformers-4.51.3\nTransformers version: 4.51.3\n\nLoading trimmed model from /kaggle/input/dataset3...\nLoading trimmed tokenizer from /kaggle/input/dataset3...\nTrimmed model and tokenizer loaded successfully!\nLoaded model vocabulary size: 2766\nLoaded tokenizer vocabulary size: 2866\nBase model moved to device: cuda\n\nDataset loaded from CSV files.\nDataset sizes:\nTrain size: 23492\nValidation size: 2937\nTest size: 2937\n\nConfiguring and applying LoRA and Adapters...\nBase model parameters explicitly frozen.\nAdapters initialized for the base model.\nAdapter 'amharic_summarization_adapter' added to the model.\nAdapter 'amharic_summarization_adapter' set as active and its parameters are trainable.\nLoRA layers added to the adapter-enabled model.\n\nTrainable parameters (LoRA + Adapters):\nTotal parameters: 48304512\nTrainable parameters: 344064\nPercentage of trainable parameters: 0.71%\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/1596524269.py:463: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FilteredArgsSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = FilteredArgsSeq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training (LoRA + Adapters on Trimmed Model)...\nNo checkpoint found, starting fresh training.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1001' max='8808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1001/8808 15:46 < 2:03:17, 1.06 it/s, Epoch 0.34/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>33.468500</td>\n      <td>9.660043</td>\n      <td>0.005114</td>\n      <td>0.000000</td>\n      <td>0.005188</td>\n      <td>0.000291</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='1322' max='1469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1322/1469 05:14 < 00:34, 4.20 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nCUDA cache cleared and gc.collect() called by callback before evaluation at step 500.\n\n--- Inside compute_metrics (DEBUG) ---\nType of eval_pred.predictions (initial): <class 'tuple'>\nType of eval_pred.label_ids (initial): <class 'numpy.ndarray'>\neval_pred.predictions is a tuple. Length: 2\nTaking first element of predictions tuple. New type: <class 'numpy.ndarray'>\nPredictions type (after initial processing): <class 'numpy.ndarray'>\nLabels type (after initial processing): <class 'numpy.ndarray'>\nWARNING: Predictions has unexpected 3 dimensions. Assuming logits and applying argmax.\nPredictions after argmax: New shape: (2937, 107), New dtype: int64\nPredictions shape (after processing): (2937, 107)\nLabels shape (after processing): (2937, 107)\nLabels processed (replaced -100).\nSuccessfully decoded 2937 predictions and 2937 labels.\nEmpty strings in decoded predictions/labels replaced with 'a' for robust metric calculation.\nLabels formatted for metrics.\nROUGE computed.\nBLEU computed.\nMetrics combined.\n--- Exiting compute_metrics ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nCUDA cache cleared and gc.collect() called by callback before evaluation at step 1000.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1596524269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;31m# Start training (pass resume_from_checkpoint if a checkpoint was found)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2628\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3094\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3045\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     def predict(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4155\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4373\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_eval_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4375\u001b[0;31m                     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         return type(tensors)(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Now let's fill the result tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.58 GiB is free. Process 2609 has 12.16 GiB memory in use. Of the allocated memory 9.01 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.58 GiB is free. Process 2609 has 12.16 GiB memory in use. Of the allocated memory 9.01 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":6}]}